{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fa8b86-7aa0-4b23-8059-0caa4315f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbbab6-7712-4055-835d-bdff8e1ed581",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading Pickle File For Know_Faces Data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2049ead0-18ab-41dc-a719-ac54e3244ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/data.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m face_pkl\u001b[39m=\u001b[39m\u001b[39mopen\u001b[39;49m(filename,\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m known_data\u001b[39m=\u001b[39mpkl\u001b[39m.\u001b[39mload(face_pkl)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/data.pkl'"
     ]
    }
   ],
   "source": [
    "filename=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/data.pkl\"\n",
    "face_pkl=open(filename,\"rb\")\n",
    "known_data=pkl.load(face_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4f0af8-1cb6-403e-8ad3-c3e628c3ccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878155f-981c-4e70-89e7-e011997e5d54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Taking pictures from the camera  source and saving it in a directory .\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c7fbc2-1aee-484a-9986-065a2d2e97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "escape hit, closing the app\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cam = cv2.VideoCapture(0)\n",
    "# making the image count as zero initially\n",
    "\n",
    "img_counter = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print('failed to grab frame')\n",
    "        break\n",
    "    cv2.imshow('test', frame)\n",
    "    #to get continuous live video feed from my laptops webcam\n",
    "    k  = cv2.waitKey(1)\n",
    "    # Here the programe waits for the key to be pressed and return the key code \n",
    "    \n",
    "    # KEY CODE OF \"ESC\" is 27\n",
    "    \n",
    "    # KEY CODE OF \"Space\" is 32\n",
    "    \n",
    "    # SO , Therfore if the escape key is been pressed, the app will stop\n",
    "    \n",
    "    if k == 27:\n",
    "        print('escape hit, closing the app')\n",
    "        break\n",
    "    # if the spacebar key is been pressed\n",
    "    \n",
    "    # screenshots will be taken\n",
    "    \n",
    "    elif k  == 32:\n",
    "        \n",
    "        # the format for storing the images scrreenshotted\n",
    "        img_name =\"capture\"+str(img_counter)+\".jpg\"\n",
    "        loc=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/\"+img_name\n",
    "        # saves the image as a png file\n",
    "        cv2.imwrite(loc, frame)\n",
    "        print('screenshot taken')\n",
    "        # the number of images automaticallly increases by 1\n",
    "        img_counter += 1\n",
    "\n",
    "# release the camera\n",
    "cam.release()\n",
    "\n",
    "# stops the camera window\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87b796f-d78c-40f3-8432-47fb971d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "pic_all=glob.glob(os.path.join(\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured\", '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04bf3ac-100f-4970-a1d7-ac46ad6c27f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture15.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture14.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture16.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture17.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture13.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture12.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture10.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture11.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture7.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture6.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture4.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture5.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture1.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture0.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture2.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture3.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture8.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture9.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture20.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture21.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture22.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture19.jpg',\n",
       " '/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/capture18.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9831fe-e662-4e07-80e1-92337035c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names=os.listdir(\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Training_images\")\n",
    "known_names.remove(\".DS_Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f1cd136-75c2-4d3f-aa42-c49b5e6778a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alekya.jpeg',\n",
       " 'simroop.jpeg',\n",
       " 'Bipin.jpeg',\n",
       " 'bhunia-min.jpg',\n",
       " 'Aditi.jpg',\n",
       " 'divyasree.jpeg',\n",
       " 'gargi.jpeg',\n",
       " 'Geetamsh.jpg',\n",
       " 'Nitya.jpeg',\n",
       " 'shravya.jpeg',\n",
       " 'nikhil.jpg',\n",
       " 'Yuraj.jpg',\n",
       " 'soham.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'Paras.jpg',\n",
       " 'anirudh.jpeg',\n",
       " 'Aritra.jpeg',\n",
       " 'Devidas.jpg',\n",
       " 'charan.jpeg',\n",
       " 'Anuj.jpeg',\n",
       " 'Sanjay.jpg',\n",
       " 'gaurav.jpeg',\n",
       " 'garima.jpeg',\n",
       " 'gopesh.jpeg',\n",
       " 'Mayank.jpeg',\n",
       " 'dhruv-min.jpg',\n",
       " 'aman_min.jpg',\n",
       " 'Piyush.jpg',\n",
       " 'jatin.jpeg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d413b33e-9fdd-4555-b7d2-5cb00957452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a8d1934-be9e-442d-a61d-cd9d4260a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognised_faces=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f36342-b860-44d3-8609-643a26122dc7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### DRAWING A BOX AROUND FACES RECOGNISED IN THE PICTURE AND OUTPUTTING THE RECOGNISED FACES IN ALL THE IMAGES\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3af9efc-0d6b-46d9-98e9-4681452c1e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to identify face locations 1.2707479000091553\n",
      "time taken to encode faces 0.2287731170654297\n",
      " \n",
      "time taken to identify face locations 1.229283094406128\n",
      "time taken to encode faces 0.20050883293151855\n",
      " \n",
      "time taken to identify face locations 1.2153081893920898\n",
      "time taken to encode faces 0.20155096054077148\n",
      " \n",
      "time taken to identify face locations 1.219857931137085\n",
      "time taken to encode faces 0.30173707008361816\n",
      " \n",
      "time taken to identify face locations 1.2164151668548584\n",
      "time taken to encode faces 0.20203375816345215\n",
      " \n",
      "time taken to identify face locations 1.218012809753418\n",
      "time taken to encode faces 0.20023822784423828\n",
      " \n",
      "time taken to identify face locations 1.2165160179138184\n",
      "time taken to encode faces 0.10038995742797852\n",
      " \n",
      "time taken to identify face locations 1.215691089630127\n",
      "time taken to encode faces 0.1998429298400879\n",
      " \n",
      "time taken to identify face locations 1.2192950248718262\n",
      "time taken to encode faces 0.19812488555908203\n",
      " \n",
      "time taken to identify face locations 1.217259168624878\n",
      "time taken to encode faces 9.489059448242188e-05\n",
      " \n",
      "time taken to identify face locations 1.2224938869476318\n",
      "time taken to encode faces 0.1994311809539795\n",
      " \n",
      "time taken to identify face locations 1.215682029724121\n",
      "time taken to encode faces 0.19909095764160156\n",
      " \n",
      "time taken to identify face locations 1.2109112739562988\n",
      "time taken to encode faces 0.1987597942352295\n",
      " \n",
      "time taken to identify face locations 1.2135050296783447\n",
      "time taken to encode faces 0.00014591217041015625\n",
      " \n",
      "time taken to identify face locations 1.2184929847717285\n",
      "time taken to encode faces 0.2027280330657959\n",
      " \n",
      "time taken to identify face locations 1.2126400470733643\n",
      "time taken to encode faces 0.19878196716308594\n",
      " \n",
      "time taken to identify face locations 1.2158849239349365\n",
      "time taken to encode faces 0.19909191131591797\n",
      " \n",
      "time taken to identify face locations 1.2136030197143555\n",
      "time taken to encode faces 0.2015669345855713\n",
      " \n",
      "time taken to identify face locations 1.225628137588501\n",
      "time taken to encode faces 0.20297718048095703\n",
      " \n",
      "time taken to identify face locations 1.224519968032837\n",
      "time taken to encode faces 0.2003469467163086\n",
      " \n",
      "time taken to identify face locations 1.2161228656768799\n",
      "time taken to encode faces 0.10019493103027344\n",
      " \n",
      "time taken to identify face locations 1.2215678691864014\n",
      "time taken to encode faces 0.30065393447875977\n",
      " \n",
      "time taken to identify face locations 1.2139501571655273\n",
      "time taken to encode faces 0.3013460636138916\n",
      " \n",
      " \n",
      " \n",
      "['Devidas.jpg', 'sujith-min.jpg', 'bhunia-min.jpg']\n"
     ]
    }
   ],
   "source": [
    "recognised_faces=[]\n",
    "for i in pic_all:\n",
    "    #starting counter \n",
    "    a=time.time()\n",
    "    img=cv2.imread(i)\n",
    "    \n",
    "    \n",
    "    face_locations = face_recognition.face_locations(img)\n",
    "    b=time.time()\n",
    "    print(\"time taken to identify face locations \"+ str(b-a))\n",
    "    \n",
    "    \n",
    "    face_encodings = face_recognition.face_encodings(img, face_locations,num_jitters=5)\n",
    "    c=time.time()\n",
    "    print(\"time taken to encode faces \"+str(c-b))\n",
    "    print(\" \")\n",
    "    img_counter=0\n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        # Try to match the face with the known face\n",
    "        matches = face_recognition.compare_faces(known_data, face_encoding,tolerance=0.5)\n",
    "        #name = \"Unknown\"\n",
    "\n",
    "        # Find the best match\n",
    "        face_distances = face_recognition.face_distance(known_data, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_names[best_match_index]\n",
    "            recognised_faces.append(name)\n",
    "            top, right, bottom, left = face_location\n",
    "            cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            cv2.putText(img, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 1)\n",
    "            cv2.imshow(\"out\",img)\n",
    "            img_name =\"capture\"+str(img_counter)+\".jpg\"\n",
    "            saved=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Detected/\"+img_name\n",
    "            img_counter=img_counter+1\n",
    "            cv2.imwrite(saved,img)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "unique_faces=list(set(recognised_faces))\n",
    "#print(recognised_faces)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(unique_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5776754-7f89-4b62-977c-bf478e2128af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bhunia-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'Devidas.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'bhunia-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'bhunia-min.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognised_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a9787-d5d7-4d2a-bb12-b630507164e9",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "# DETECTING ALL FACES IN THE FRAME \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc69707-8c84-44a4-b980-99c18167d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to identify face locations 13.18276596069336\n",
      "time taken to encode faces 127.57230234146118\n",
      " \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'recognised_faces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m#if matches[best_match_index]:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m name \u001b[39m=\u001b[39m known_names[best_match_index]\n\u001b[1;32m---> 43\u001b[0m recognised_faces\u001b[39m.\u001b[39mappend(name)\n\u001b[0;32m     44\u001b[0m top, right, bottom, left \u001b[39m=\u001b[39m face_location\n\u001b[0;32m     45\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(img, (left, top), (right, bottom), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recognised_faces' is not defined"
     ]
    }
   ],
   "source": [
    "filename=\"encoding_for_ic240.p\"\n",
    "face_pkl=open(filename,\"rb\")\n",
    "ll=pkl.load(face_pkl)\n",
    "known_data , known_names = ll\n",
    "face_pkl.close()\n",
    "\n",
    "fdpath=\"abcde\"\n",
    "pathlist = os.listdir(fdpath)\n",
    "pic_all=[]\n",
    "\n",
    "for path in pathlist:\n",
    "    pic_all.append(cv2.imread(os.path.join(fdpath,path)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for img in pic_all:\n",
    "    #starting counter \n",
    "    a=time.time()\n",
    "\n",
    "    \n",
    "    face_locations = face_recognition.face_locations(img)\n",
    "    b=time.time()\n",
    "    print(\"time taken to identify face locations \"+ str(b-a))\n",
    "    \n",
    "    \n",
    "    face_encodings = face_recognition.face_encodings(img, face_locations,num_jitters=5)\n",
    "    c=time.time()\n",
    "    print(\"time taken to encode faces \"+str(c-b))\n",
    "    print(\" \")\n",
    "    img_counter=0\n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        # Try to match the face with the known faces\n",
    "        matches = face_recognition.compare_faces(known_data, face_encoding,tolerance=0.5)\n",
    "        #name = \"Unknown\"\n",
    "\n",
    "        # Find the best match\n",
    "        face_distances = face_recognition.face_distance(known_data, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        #if matches[best_match_index]:\n",
    "        name = known_names[best_match_index]\n",
    "        recognised_faces.append(name)\n",
    "        top, right, bottom, left = face_location\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        #cv2.putText(img, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 1)\n",
    "        cv2.imshow(\"out\",img)\n",
    "        img_name =\"capture\"+str(img_counter)+\".jpg\"\n",
    "        saved=\"abcde/\"+img_name\n",
    "        img_counter=img_counter+1\n",
    "        cv2.imwrite(saved,img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2821c746-1fc9-4ed8-ab98-0e262f102d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aman_min.jpg', 'bhunia-min.jpg', 'sujith-min.jpg']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognised_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53478d3d-1bc7-4c23-87ed-46bc93d5357d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
